# -*- coding: utf-8 -*-
"""Adult_fnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eUv5DKbJ3l1IdeVTZcPm6AiWMQegoUoc
"""

import sys
!{sys.executable} -m pip install ucimlrepo

from ucimlrepo import fetch_ucirepo
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split

# fetch dataset
data = fetch_ucirepo(id=2)

# data (as pandas dataframes)
X = data.data.features
y = data.data.targets

cleaned_indices = X.dropna().index
X.dropna(inplace=True)
y = y.loc[cleaned_indices]

y.replace(("<=50K", "<=50K."), 0, inplace=True)
y.replace((">50K.", ">50K"), 1, inplace=True)

#Drop Columns
X.drop(["fnlwgt","education-num"], axis=1, inplace=True)

# Assuming 'categorical_columns' and 'numerical_columns' are lists of column names
categorical_columns = ['workclass','education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country', 'sex']
numerical_columns = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']

# Creating a ColumnTransformer
preprocessor = ColumnTransformer(transformers=[
        ('ord_encoder', OrdinalEncoder(), categorical_columns),
        ('std_scaler', StandardScaler(), numerical_columns)
    ])

# Creating a pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    # Can add more steps in the pipeline
])

# Fit and transform your data
X_preprocessed = pipeline.fit_transform(X)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.3, random_state=0)

# Define the FNN model
model = Sequential()

# Add input layer and hidden layers
model.add(Dense(units=128, activation='relu', input_dim=X_train.shape[1]))
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=32, activation='relu'))

# Add output layer
model.add(Dense(units=1, activation='sigmoid'))  # Assuming binary classification, adjust units for multiclass

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test))

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')